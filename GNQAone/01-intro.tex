\section{Introduction}


\noindent Scholars world wide spend exhaustive amounts of time and effort reading, summarizing, converting, memorizing, and mixing knowledge for their own purposes, and to push science forward.
Since the inception of AI its promise has exponentially out-sized its capabilities; however, computing resources strong enough to exploit deep learning in conjunction with continual improvements in the use and management of `big data' have begun to close the gap between `perceived utility' and promise of AI.
`Perceived utility' is how the common person understands AIs use and helpfulness.
There are AI utilities, applications, and algorithms that are used widely for automation, recognition, navigation, semi-autonomous vehicles, mars rover exploration, deep question answering, and now creativity.
The essence of AI is getting computers to perform `intelligent' human tasks.
Pursuit of the same has caused some researchers to thoroughly examine and re-examine their definitions of intelligence.
Many see a humanoid android or automaton that is difficult to differentiate from a human as the pinnacle of AI; because AI as a field has so many areas in which it needs to improve to reach such a technical height, it is defined with many sub-fields.
The major sub-fields of AI include: machine learning, natural language processing, 
\cite{Azaria:2022} \cite{Zhang:2023} \cite{Foucart:2023} \cite{DePeau-Wilson:2023}

Since the deep learning boom in 2010, there has been a mad rush to apply and improve its techniques.
The first `killer' application was image classification, still and video.
Deep learning has made image classification so accurate until a large technology company has made it known that it has failed unless its `driverless' technology is the hallmark feature in its electric vehicles. 
Before the success of deep learning models on images classification attempts at `self-driving' vehicles failed or the idea was a `non-starter`.
As we tend to only hear about the most negative news, the existing Tesla `self-driving' cars are making moves on the road, mostly to the great delight of their owners.
Deep learning techniques have made the accuracy and error rate of many biometric technologies so enviable that many scientists consider some biometrics issues as solved, especially with modalities that are highly persistent, permanent and ubiquitous (e.g. iris, face, fingerprint).
As deep learning is being improved so goes the hardware used to run the necessary algorithms.

Consistently improving techniques and hardware leads to neural networks with billions of parameters instead of hundreds or thousands.
In the same vein, it seems that it takes a model with billions of parameters to really `understand' and respond using human language.

By training and painstakingly moderating humongous neural network modesl, OpenAI, was able to build what seems a quantum leap in the generative AI, ChatGPT a large language model that generates `mostly coherent' responses to human speech, without having anything close to a human curated script.
Although ChatGPT relies on a neural network model to accomplish its goals, it has popularized the term `large language model' (LLM).
In addition to creating a model that is magnificent at parsing information, understanding free-form human requests, and responding in a manner that can lead to the easy passing of the Turing test, OpenAI and others have trained neural network models on so many examples of data in different areas until `Generative AI' has pervaded the popular lexicon.
Generative AI (gAI) uses deep learning techniques to produce images, video, text, and more based on a free form query, making ChatGPT itself gAI.
Generative AI has recently gained attention due to the ability of services such as Midjourney (get ref) generating professional quality logos, portraits and other images based off of requests in free form human language.
The term `free form' is being used often as the generative models and LLMs do not need one to follow a script to generate output.

\begin{comment}
Thankfully AI is still an extremely hot topic; however, people are referring to everything that seems computationally smart as `artificial intelligence' (AI), mainly to keep things simple.
As stated in the introduction deep learning, mainly based on convolutional neural networks (CNNs), began showing breakthrough performance in image classification tasks, and scientists have been having a field day with the performance gains for multiple tasks, even in genomics [cite so many previous papers from the Diversity application write-up].
\end{comment}