\section{Current State-of-the-Art}

Artificial intelligence is a large research area that includes machine learning, and expert system topics~\cite{RussellNorvig:2016}.
Expert systems have two major components: an inference engine and a knowledge base~\cite{Liao:2005,Tan:2017}.
Machine learning models are best defined by their knowledge base, network architecture, activation function and number of parameters.
Machine learning techniques hae been applied to many areas of biological research \cite{Deng:2011,Angermueller:2016,Gawehn:2016_deep,Min:2017,Miotto:2018:deep,Tran:2018,Toh:2019,Zhang:2019,Bouwmeester:2020,Wen:2020:deep,Caudai:2021,Talukder:2021,Hassabis:2022} for the better part of the last decade.

\subsection{Large Language Models (\llms)}
Late 2022 saw an introduction to \llms\ that got the notice of the entire world \cite{Schulman:2022_chatgptintro}.
OpenAI debuted ChatGPT an `ask me anything' system that seemed to be the logical extension of good `search' and excellent `prompt' support, in the veins of an incredibly improved `Alexa' (by Amazon), `Siri' (by Apple) or `Cortana' (by Microsoft).
While these voice activated assistants seem `intelligent', they are programmed in a fashion that allows them to support very specific and directly encoded tasks, such as controlling `smart' devices (e.g. thermostats, lights, alarms, locks, speakers and televisions) \cite{Hoy:2018}.

Google Bard \cite{GoogleBard:2023}, Bing Chat \cite{BingChat:2023}, Dolly by Databricks \cite{Dolly:2023}, and Llama by Meta \cite{Llama:2023,Llama2:2023} are `big techs' answer to the previously rogue startup `OpenAI', as Microsoft has recently invested heavily \cite{Browne:2023}.
Many of the weights of the models are kept under wraps by their owners; however, Databricks and Meta have open-sourced their models, allowing people all over the world to experiment and extend a high quality LLM democratizing the technology.
OpenAI allows for a paid account to fine-tune their models for specific use-cases, but it has not open-sourced the model to enable others to run a full OpenAI \gpts\ model for themselves.
We will build and test using the open-source \llms.


\subsection{Causal Learning}
In their explanatory reference summary book for causal inference (CI), Pearl and Mackenzie~\cite{Pearl:2018} make the observation that machine learning algorithms and tools can `at best' provide a summary of data and/or transform it without using a model (of the precise problem domain).
In ML a `model' is an algorithm or set of algorithms trained to recognize a very limited sub-class of pattern.
It refers to a replica of the environmental and situational conditions that lead to an observable outcome or data at a manageable scale.
CI describes its usefulness using the `ladder of causation', which has three levels: association, intervention and counterfactuals \cite{Pearl:2018}.
An example of questions that can be answered by association:
\begin{itemize}
    \item What does a sore throat tell me about the severity of the flu?
    \item If I experience dizziness does that necessitate inner ear trouble?
\end{itemize}

Example questions that can be answered by intervention:
\begin{itemize}
    \item If I drink hot water with lemon and ginger will my cold symptoms disappear?
    \item Will taking the same Covid-19 vaccine 3 times ensure I'll never have "severe" Covid-19 symptoms?
\end{itemize}

Example questions that can be answered by deploying counterfactuals:
\begin{itemize}
    \item If I had done heavy exercise 6 times a week for the past decade, would I be an Olympic level athlete?
    \item If I had not been exposed to high levels of mosquito repellent and steel soot as a young child, would I still have asthma?
\end{itemize}

At the second level of the ladder of causation, intervention, the base data alone can not answer the question.
For instance, data can answer that many people reported feeling relief from cold symptoms, and tell us the percentage, possibly of people who gel better who have profiles similar to oneself.
Intervention questions are usually answered using experimentation, while with CI these questions will be answerable using its techniques.

As we found in the summary of deep learning in omics reviews people need to trust the data, algorithms and techniques used for translational medicine, phenotype prediction, precision medicine, advanced therapeutics and more.
A good way to understand is by asking ``why'', and as Judea Pearl \cite{Pearl:1995,Pearl:2009,Pearl:2019} has explained in his multiple works is by using causal inference.
It can argued that without a system being able to answer questions properly that include interventions and counterfactuals, the system can not truely have human level intelligence.
Computers have for a long time had better `memory' and `memory access speed' than humans, a fact proven by IBM's Watson in 2011~\cite{Ferrucci:2010_watson}; however, even given all of the data in the world, the current LLM based inferencing and optimized search and retrieval does not allow systems to answer counterfactuals and many non-popular intervention queries.
Therefore, in order to push \llms\ to an even more impressive path of meaning generation and inference is by including causal reasoning into the training or fine-tuning of huge foundation models.
