\section{Weaknesses in LLMs}
A major problem with \llms\ is the unproven high level of trust placed in them by excited users.
A small anecdote referneces a genomics Ph.D. student who told me in early 2023 that ChatGPT is close to `perfect' as it responds in a manner she deems favourable to more than 90\% of her prompts.
This statement, although from a Ph.D. student was without qualification or data.
It is likely the use cases to which her prompts belong were in the list of those ChatGPT handles well, and she was possibly not an expert on the subjects with which she prompted the system.
Data is not referenced when answers are generated.
LLMs and generative AI have the same issues in that pure fact patterns are not mapped, similarities are mapped and based on the data available \cite{Floridi:2023}.
While the model is being trained it learns, and in the case of \gpts\ class models learns well, associations and not facts.

\subsection{Bias perpetuation}
Research assumptions and findings are based on the data available, which has been found is often not representative of the data that exists.
Statistical analysis is utilized in an attempt to find a population sample size that is representative of a problem space, and extrapolate findings based off of equations that scale the findings from the sample to the whole population.
Unfortunately this is not applicable in all scientific scenarios.
Say for instance you are developing facial recognition algorithms and you determine a representative sample size for your study is one thousand people for a population size hundred times larger.
Statistically the sample size may be representative, by the numbers; but if demographic representation is not considered, regardless of the validity of the studies methods, the sample size of nine hundred and ninety faces of those with mainly European ancestry while only having ten of African or Native American ancestry, the facial recognition algorithm will become tuned to best recognize the faces best represented in its sample.
Biases are present in almost all research datasets, and those who work with these datasets must be aware of its shortcomings, so as to not perpetuate the biases invisibly with further works.

\subsection{Reconstruct protected data}
The internet was scoured for training material for the OpenAI \gpts. 
Hence, upon receiving a prompt to create a narrative along the lines of an existing one, on which the system has trained, the generated content can recreate something copyrighted, patented, or protected in some other legal fashion.


\subsection{Information hallucination and Logic errors}
\llms\ are not yet ready to be relied upon in mission critical situations because they can hallucinate \cite{OpenAI:2023:gpt4}.
As the populace tests the ChatGPT platform they have put it to the task of generating creative scientific work while referencing its sources.
Unfortunately, more often than not the \gai\ hallucinates and generates sources rather than citing ones that exist.
Similarly when faced with a prompt that requires the system to follow logic to craft a useful response, the system still may fail.
This is not due to poor performance, just the nature of how \llms\ work and use data ingested to generate human readable responses.
\llms\ are not logic engines, they are learning engines that mimic human language using big data.

 
\subsection{Little to no explainability}
Given that \gpts\ are \llms\ wit billions of parameters, attempting to understand the models reasoning is close to an impossibility.
In the field of explainable AI, tchniques exist to support neural network explainability; however, they do not scale well to neural networks with billions of parameters.

%\subsection{Susceptibility to prompt injection}

\section{Mitigating the weaknesses of an open LLM}
The existing proof of concept utilizes a GPT3.5 fine-tuned system.
OpenAI has identified several of their technical issues and are implementing corrective measures and/or are allowing the system to improve incrementally as it is used \cite{OpenAI:2023:gpt4}.

In genetics and omics research we must address bias perpetuation with respect to our datasets, and the system implementation and design cannot fix the bias that comes from  decades of creating datasets based on their historic and current prejudice.
Mitigating the bias can be done thru adding a module that checks experiment meta data to list a demographic summary when returning results including human studies and datasets.

\subsubsection{How much data?}
ChatGPT was trained on multitudinous data scoured from all over the internet. 
With this almost indeterminate information grab comes petabytes of information, which required extreme effort to curate.
\subsubsection{Which data?}
Is all data good data?
Due to the nature of the world wide digital web (www) \textbf{bias} is prevalent, and training a model on highly biased information leads to biased results [mention the algorithmic justice league].
\subsubsection{Re-introducing Oracles}
In simple language a software Oracle is like an advanced lookup/hash table or data tuple where each entry has a single matter-of-fact answer.
An Oracle is generally a master of a special area of knowledge.
In the real world, an Oracle is an expert.
Medical practitioners are Oracles; however, they are Oracles with specializations concerning how different systems of the human body works and how it responds to multitudinous treatments.
There are many different types of medical practitioners, one group of which are nurses.
Nurses have different levels of expertise on the human body, how people react to treatment, real-life human-drug interactions, and sub-specialties, e.g. Renal nurse, Mid-wife (birthing) nurse, pediatric nurse, neonatal ICU nurse, emergency room nurse, oncology nurse, surgery nurse, post-surgery nurse, and so on.
Each nurse has different expertise that spans not only written medical knowledge, but also experience from interacting face-to-face with living patients.
People may have the same major malady yet need to be given differing treatments based on their personality traits, not any other underlying medical conditions.
Being able to encode this knowledge into an Oracle is nothing new, while it takes care and attention [cite many expert systems].
Part of the inference of an expert system is being able to, for example, take malady's and the knowledge of nurses to come up with the best treatment for an individual.
Also, it would be wonderful to be able to know how often a prescribed treatment works, and how well.

\begin{comment}
Each nurse has undergone specialized training to become and expert and then learned thru practice.
In nursing practice there are many variables involved when discerning and providing treatment.
A model is fed with knowledge, like a nurse being trained, and the models' practice is interacting with experts and being told whether or not the answers it is giving are correct given the query.
\end{comment}