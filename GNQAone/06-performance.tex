\section{Evaluation \& Performance}

The system is a complex question and answer system, as defined by \cite{Daull:2023:complex}.
The performance of such a system needs to be measured quantitatively and qualitatively.
Qualitative measures such as usability and usefulness seem obvious and not necessary to evaluate, as it is a \gpts\ type of system, where such systems have a question answering functionality and has seen enormous use since its popular debut in late 2022.
However, the \project\ is an expert system for a very specific domain, which will act as a research companion.
To this end the normal qualitative performance factors, e.g. the system usability scale as described by \cite{Blattgerste:2022}, must be evaluated for this system and domain.

%Speak to the qualitative and quantitative shortcomings of the system as it exists.
%Some shortcomings are in the GUI and others have to do with how the model is tuned, and now there are some improvemets our collaborators need to make.

As the work moves on system performance will be measured similarly to \cite{Daull:2023:complex}, by using:
\begin{itemize}
    \item speed of inference
    \item multi-document summarization
    \item biases reduction
    \item how answers compare to large non-specilized \llms\
    \item Usefulness for biologists
    \item Usefulness for non-biologists
    \item usability
    \item ease-of-use
    \item safety and data sensitivity, truthfulness, veracity and hallucinations
    \item alignment to human intentions, expectations and values
     %and receiver operating characteristic (ROC) curve, .
\end{itemize}

Some performance measures like, speed of inference and multi-document summarizaiton and biases reduction, will not be evaluated, until w begin using an open source model hosted at our institution for training.
At the moment we are fine-tuning an OpenAI model.
OpenAI models, as their models have been the main generators of excitement for large language models, will be treated as the default benchmark against which future work will be compared.

Our proof-of-concept system has been tested by a subject matter expert, and his comments on the system's ability to answer questions in fine-tuned domain show us several improvement areas, including: concerns with hallucination, knowledge of specific genes and core topic knowledge, retaining query context, the breadth of a summary given the references from which it is said to come, and generating responses not as strong as those of the foundation model.
The most recent OpenAI models have not conquered hallucination \cite{OpenAI:2023:gpt4} and this is why we have a design that does not fully rely on the LLM to generate the truth from what it has ingested.

\subsubsection{Lack of Knowledge}
Some of the biologists comments that question the systems knowledge follow: 
\begin{quotation}
\cyan{What is the most recent paper published using the BXD population?}

\textbf{The most recent paper published using the BXD population is from 2018. The most recent paper published using the BXD population is from 2019.} 

\textit{I am assuming the two different responses (some days apart) are because the database of papers is being actively expanded. I can’t actually verify the LLMs statement as the citation information is not exposed yet (just a hash). The top hit quotes from David’s ‘expanded BXD family’ paper from 2019, so it seems OK. Not sure why there is more than one hit for this query though (is the LLM actually collecting dates and finding the max? Or is it just making up text?)}

\cyan{How many strains are available for the BXD population?}

\textbf{There are a totoal of 123 BXD strains available.} 

\textit{I have received some different values, but all under the 150 quoted in David's 2019 paper (which we know from above is in the reading list).}


\cyan{What is the typical weight of a mouse liver?} 
\textbf{The average liver weight in a normal mice is 10 mg per g wet weight.}  
\textbf{The average weight of a mouse liver is 3.5 grams.}
 
\textit{Two very different answers on different days. Both wrong. I thought this would be one that an LLM could deal with.}
\end{quotation}

\llms\ are known to have problems with answering questions like `What is the time?', when there is no `oracle/expert' present to provide the precise value.
The disappointment with the system not being completely correct on specific facts that may be found in the uploaded literature is understandable and fixable with up-to-date information and an `oracle/expert' that can provide the answer.

\begin{quotation}
\cyan{What is the NCBI GeneID four mouse ATf3?}

\textbf{The NCBI GeneID for mouse Atf3 is ENSMUSG00000027109.}

\textit{Close, but no cigar. We got a gene ID for a mouse transcription factor, but that's all. ENSMUSG00000027109 is mouse Sp3.}

\cyan{What is the ENSEMBL GeneID for mouse Atf3?}

\textbf{The ENSEMBL GeneID for mouse Atf3 is ENSMUSG00000064371.}

\textit{Nope, this is mitochondrial tRNA threonine. I wasn’t expecting much from this query as we know this is NOT what LLMs do well. Maybe there could be re-training on the NCBI/Uniprot descriptions and even GeneRIF. I still think there should be a way to directly link entities to databases though.}

\cyan{What is the ENSEMBL identifier for Atf3 in mice?}

\textbf{The ENSEMBL identifier for Atf3 in mice is ENSMUSG00000027109.}

\textit{Rephrasing the question gives qualitatively different answers. The citations are all kind of sane though. They actually do get close to Atf3 and the IDs appear to be drawn from these sources. Some explicit grounding would nail this down I think.}
\end{quotation}

In addition to creating oracles, we must identify the correct databases and knowledgebases.
From there we create a vector representation of the information and fine-tune it on links to external resources, similar to how BingChat~\cite{BingChat:2023} operates.

\subsubsection{Query context}
%Another issue found with the proof-of-concept is lack of continuing knowledge or the system is not retaining context of previous queries.
%This is due to OpenAI's API not doing the same automatically, although the chat interface does.
Others have run into this issue and automatically concantenate all previous or multiple previous prompts together, and this seems to be not optimal.
Grouping queries by similarity or topic is something an LLM or other model can perform, then prompt with context as understood by a secondary model.
One reason the biologist may have expected context is because ChatGPT's chat function maintains the same, and \project\ can be made to behave similarly.

\begin{quotation}
\cyan{What are these QTLs?}

\textbf{These QTLs are stretches of DNA that are closely linked to genes that underlie a phenotype of interest.} 

\textit{I was hoping for more information about the previous query. I got basically the same response when asking the same question in a different session though, so there is no evidence of contextual memory. I didn’t pursue this further.}
\end{quotation}

Concerning the retention of query context, we can test adding automatic text propagation for queries that are similar then submit a better more encompassing question to the model, and ask the model the latest question separately to provide multiple answers, one that takes into account a previous query, and one that is standalone.

\subsubsection{Strength of Response and Summarization}

\begin{quotation}
\cyan{How are C57BL/6 and DBA/2 different?}

\textbf{C57BL/6 and DBA/2 differ in their ability to respond to xenobiotics.} 

\textit{I saw some different responses, all quoed from single references - not a summary as I would have liked to see.}
    
\cyan{What behavioural QTLs have been found in mice?}

\textbf{Two QTLs located on chromosomes 1 and 5 modulate different aspects of the performance of mice of the B x D Ty RI strain series in the Morris navigation task.} 

\textit{Sounds like it has been ripped verbatim from a single paper. Edit: Yes, this is from reference \#3.“Two QTLs located on chromosomes 1 and 5 modulate different aspects of the performance of mice of the BXD Ty RI strain series in the Morris navigation task. Behav Genet. 2002; 32:6978. [PubMed: 11958544] Mozhui RT, Ciobanu DC, Schikorski T, Wang XS, Lu L, Williams RW. Dissection of a QTL hotspot on mouse distal chromosome 1 that modulates neurobehavioral phenotypes and gene expression. PLoS Genetics. 2008; 4:e1000260. [PubMed: 19008955] Mulligan MK, Wang X, Adler AL, Mozhui K, Lu L, Williams RW. Complex control of GABA(A) receptor subunit mRNA expression: variation, covariation, and genetic regulation. PLoS One. 2012; 7(4):e34586.” Here, it looks like a part of the References section has been used in the text mining. That is a problem if the aim mis to link statements to correct citations. I don’t know what the source reference is (still just a hash) but it is very likely NOT the source of the response text.}

\cyan{What is the role of Atf3 in the brain?} 

\textbf{Atf3 is a transcriptional activator that binds to the cAMP-responsive element (Cre). Interaction with Jun redirects Jun to bind to Cres preferentially over the 12-Otetradecanoylphorbol-13-acetate response elements (Tres) as part of an Atf2-c-Jun complex (487 aa). However, human study did not show strong support for the association between ATF2 and alcoholism. Alcoholism is particularly influenced by the environment.}

\textit{The response seemed to come only from individual studies rather than being a summary of the field. That may be due to the small post-training dataset at this stage. Is there a 'relevance score' for the papers returned? Several references (and part of the response) were based on Atf2. This is a different gene. Does the system perform grounding to biomedical ontologies? Or is it just looking at text similarity (like Levenshtein distance)?}
\end{quotation}


\begin{quotation}
\cyan{Where can I find the ENSEMBL identifier for a gene such as Atf3?} 

\textbf{The ENSEMBL identifier for a gene can be found in the Affymetrix (June 2005 Annotation) and Ensembl AffyProbe (v33) databases.}

\textit{Hmm - yeeees, you COULD find the identifier there. But that is not where we should be looking. As a comparison, I ran the same query through ChatGPT (3.5):}
\red{\textit{As of my last knowledge update in September 2021, the gene Atf3 is indeed present in the ENSEMBL database, but please note that information might have changed since then. To find the ENSEMBL identifier for the Atf3 gene, you can follow these steps: 
    1. Visit the ENSEMBL website: Go to the ENSEMBL website at https://www.ensembl.org/. 2. Search for the gene: In the search bar at the top of the page, enter "Atf3" and press Enter or click the magnifying glass icon. 3. Select the gene: From the search results, you should find "Atf3" listed. Click on the gene name to access its detailed information page. 4. Find the ENSEMBL identifier: On the gene's page, you should be able to find the ENSEMBL identifier (also known as the ENSEMBL Gene ID or ENSGID) listed in the gene's summary information or on the "Gene" section of the page. Please note that the specific layout of the ENSEMBL website and the location of information might have changed since my last update. If you're having trouble locating the ENSEMBL identifier or if you want the most up-to-date information, I recommend visiting the ENSEMBL website directly and using their search and navigation tools to find the gene Atf3 and its corresponding ENSEMBL identifier.}}

\cyan{Where do I look for genes that regulate hippocampal neural stem cell proliferation?}

\textbf{In the QTL region.}

\textit{Obviously :D. I was expecting something more along the lines of the ChatGPT response, that pointed me (in a generic way) to a bunch of sensible resources.}

\cyan{What does QTL mean?}

\violet{Quantitative trait locus (QTL) is a gene or group of linked genes that influence a quantitative trait.}

\textit{OK, but weak answer given that this is exactly the sort of situation wher an LLM should shine. Would it perhaps make sense to give the LLM the FAQ data as part of its training?}
\end{quotation}

To reiterate we have fine-tuned the model with nine hundred fifty research documents related to \GN, which may have not created a knowledge base deep enough for richer responses.
The ways to improve the richness of response is by asking the API to provide richer feedback, have the system ingest more data in the area of interest, fine-tune the model on more metadata generated over the last two and half decades of \GN\'s existence, and to reference other supporting data sources (including links).
All of these different modifications will be made and evaluated.

% the same is modified to be different than the model trained with the domain information.

\subsection{More room for improvement}
Based on feedback from a biologist, as seen in the aforementioned quotations, we find the more pertinent performance factors for our use case include: alignment to human intentions, expectations and values, along with comparison to non-specialized \llms\, and usefulness for biologists.
%Based on feedback (table~\ref{tab:rupertsfeedbackp1} and table~\ref{tab:rupertsfeedbackp2}) from a biologist we find the more pertinent performance factors include: alignment to human intentions, expectations and values, along with comparison to non-specialized \llms\, and usefulness for biologists.
A measure like biases reduction must be properly defined for it to have meaning.
We build this expert system with a specific community in mind; hence, the biases toward genetics, genomics should be high.
However, as is commonly found in research, researchers study groups with which they have the most experience, which leads to diversity biases, as the genetics and genomics field is most represented by those of European and East Asian descent.
Medical research suffers from non-conscious stereotyping and prejudice, a most persistent legacy of slavery and Jim Crow in the United States of America~\cite{Stone:2011}.
Hence, for this system we must provide a caveat to users that speaks to the history of medical research, and then present facts on which the system will be fine-tuned.
    

After obtaining feedback from multiple biologists familiar with \GN\ and its ecosystem a list of questions will be developed that is representative of what a biologist thinks the system should perform well. 
The same will be turned into a digital survey, similar to work by \cite{Blattgerste:2022} that can be taken from a link on the site when the system is ready for further analysis.

Once we move into phase three for the development of \project\ the question of usefulness will be the most important to answer.
When functonality is added, it is best to design it to supplement and support the \project\ and not overshadow it.
\GN\ is a large database that contains data from multiple decades of research that has been used for research experimentation.
When one asks about a specific experiment of class of experiments, being able to recreate the digital environment that supported the analysis is of paramount importance, and brings us back to the benefits of \guix.
The \project\ should be able to give instructions and links to data and software, along with instructions on how to recreate a study and links to data, metadata and other findings.

In addition to the list of performance metrics above, we look to Standford University's holistic evaluation for language models (HELM) benchmark as described by \cite{Liang:2022:helm}.
\llms\ are poised to replace regular internet `search', as evidenced by Microsoft's bing chat~\cite{BingChat:2023} combine their search functionality with their proprietary LLM to provide a more verbose and helpful interface than either service provides alone.
HELM is being looked at because it takes a human-centered approach to LLM evaluation, which we want to follow as we work in the field of translational medicine and ar building a platform that should be trustworthy and helpful to people by design, not just correct.
\llms\ are being implemented so quickly that there are no regulations made to protect against misuse of the technology, and the system we are building should be an example of \llms\ being used correctly for the broadest imaginable swath of people and use cases, while being malleable to conform to different cultural norms and use cases on the edge.

Evaluating \project\ will require a modification of HELM, as the issues of greatest importance to the operation of this expert system are scientist/human-centered perspectives of operation and overall correctness.
Our expert system needs to support research, and grease the wheels of solutioning.

%This list will be tested among multiple system configurations and wth different platforms to come up with false positives, true positives, and other categories that will allow us to use the ROC curve.

