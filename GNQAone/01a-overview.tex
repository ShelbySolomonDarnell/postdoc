\section{Overview}


\noindent Scholars world wide spend exhaustive amounts of time and effort reading, summarizing, converting, memorizing, and mixing knowledge for their own purposes, and to push science forward.
Since the inception of AI its promise has exponentially out-sized its capabilities; however, computing resources strong enough to exploit deep learning in conjunction with continual improvements in the use and management of `big data' have begun to close the gap between `perceived utility' and promise of AI.
`Perceived utility' is how the common person understands AIs use and helpfulness.
There are AI utilities, applications, and algorithms that are used widely for automation, recognition, navigation, semi-autonomous vehicles, mars rover exploration, deep question answering, and now creativity.
The essence of AI is getting computers to perform `intelligent' human tasks.
Pursuit of the same has caused some researchers to thoroughly examine and re-examine their definitions of intelligence.
Many see a humanoid android or automaton that is difficult to differentiate from a human as the pinnacle of AI. 
AI as a field has so many capability areas which require improvement to reach such a technical height, which according to \cite{RussellNorvig:2016} include: knowledge representation, automated reasoning, natural language processing, and machine learning  
\cite{Azaria:2022,DePeau-Wilson:2023,Foucart:2023,Zhang:2023}.


Since the deep learning boom in 2010, there has been a mad rush to apply and improve its techniques.
The first `killer' application was image classification, still and moving.
Deep learning has made image classification so accurate until a large technology company has made it known that it has failed unless its `driverless' technology is the hallmark feature in its electric vehicles \FIXME{Ref: Find EMusk quote on the same}. 
Before the success of deep learning models on images classification attempts at `self-driving' vehicles failed or the idea was a `non-starter`.
As we tend to only hear about the most negative news, the existing Tesla `self-driving' cars are making moves on the road, mostly to the great delight of their owners.
Deep learning techniques have made the accuracy and error rate of many biometric technologies so enviable that many scientists consider some biometrics issues as solved, especially with modalities that are highly persistent, permanent and ubiquitous (e.g. iris, face, fingerprint) \FIXME{find appropriate references}.
As deep learning is being improved so goes the hardware used to run the necessary algorithms.

Consistently improving techniques and hardware leads to neural networks with billions of parameters instead of hundreds or thousands.
In the same vein, it seems that it takes a model with billions of parameters to really `understand' and respond using human language.

By training and painstakingly moderating humongous neural network models, OpenAI~\cite{Bavarian:2022,Weng:2022} was able to build what seems a quantum leap in the generative AI, GPT4~\cite{OpenAI:2023_gpt4} a large language model (LLM) that generates `mostly coherent' responses to human text queries, without having anything close to a human curated scripts. \FIXME{reference previous voice apps and their system architecture}.
Although ChatGPT relies on neural network to accomplish its goals, it has popularized the term `large language model' (LLM).
In addition to creating a model that is magnificent at parsing information, understanding free-form human requests, and responding in a manner that can lead to the easy passing of the Turing test, OpenAI and others have trained neural network models on so many examples of data in different areas and ChatGPT is being used popularly until the term `Generative AI' has pervaded the popular lexicon.
Generative AI (gAI) uses deep learning techniques to produce images, video, text, and more based on a free form query, making ChatGPT itself gAI.
Generative AI has recently gained attention due to the ability of services such as Midjourney \FIXME{get reference} generating professional quality logos, portraits and other images based off of requests in free form human language.
The term `free form' is being used often as the generative models and LLMs do not need one to follow a script to generate output.

\begin{comment}
Thankfully AI is still an extremely hot topic; however, people are referring to everything that seems computationally smart as `artificial intelligence' (AI), mainly to keep things simple.
As stated in the introduction deep learning, mainly based on convolutional neural networks (CNNs), began showing breakthrough performance in image classification tasks, and scientists have been having a field day with the performance gains for multiple tasks, even in genomics [cite so many previous papers from the Diversity application write-up].
\end{comment}